{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pure python neural network model of reflex conditioning in animal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to add instructions on\n",
    "\n",
    "install bleeding edge lasagne & theano\n",
    "are we to assume that if they can open the notebook they are good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the following packages:\n",
    "* [Lasagne](http://lasagne.readthedocs.io/en/stable/index.html) :: A neural network framework built on Theano\n",
    "* [Numpy](http://www.numpy.org/) :: Scientific computing packaging\n",
    "* [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) :: Data manipulation, munging, formatting library\n",
    "* [Theano](http://www.deeplearning.net/software/theano/) :: Deep learning framework\n",
    "* [Matplotlib.pyplot](https://matplotlib.org/api/pyplot_api.html) :: Visualization\n",
    "\n",
    "Begin by importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\users\\adam\\docume~1\\python~1\\projects\\nn_pro~1\\nn_proj\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If a warning DEPRACATED populates, set a USER variable defined as THEANO_FLAGS to an empty string.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constansts for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################## Define Constants #################################\n",
    "N_CS = 5\n",
    "N_CONTEXT = 10\n",
    "N_SAMPLES = 25\n",
    "N_BATCHES = 250\n",
    "N_SIMS = 20\n",
    "output_file = 'output.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function builds the dataset or input vector.  The dataset is a 2-dimensional binary array consisting of an initial 5 element CS, or *conditioned stimulus* portion and a 10 element context portion.  The length of each piece, and thus the dataset, is controlled using constants defined above. The `cs[rand_num][0] = 1.0` portion sets the value of the 1st element of a random vector within the array to 1, this correlates to *stimulus present*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(n_cs=N_CS, n_context=N_CONTEXT, n_samples=N_SAMPLES):\n",
    "    # build out cs portion of input var\n",
    "    cs = [[0 for i in range(n_cs)] for j in range(n_samples)]\n",
    "    rand_num = np.random.randint(0, high=len(cs))\n",
    "    cs[rand_num][0] = 1.0\n",
    "\n",
    "    # build out context portion of input var\n",
    "    context = [float(np.random.randint(0, high=2)) for i in range(n_context)]\n",
    "\n",
    "    # build input var\n",
    "    input_var = []\n",
    "    for array_item in cs:\n",
    "        input_var.append(array_item + context)\n",
    "    return np.asarray(input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, defining a function to build targets for a loss function within the model.  The previously built dataset is used a reference point when building the targets to mitigate any errors while running the loss function.  The `cs_index = targets.index(1.)` saves the index of the *stimulus present* vector within the larger dataset array for use in later parts of the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(input_var):\n",
    "    targets = []\n",
    "    for item in input_var:\n",
    "        if np.any(item[0] == 1.0):\n",
    "            targets.append(1.0)\n",
    "        else:\n",
    "            targets.append(0.0)\n",
    "    cs_index = targets.index( 1.)\n",
    "    return np.asarray(targets), cs_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to building the network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to use lasagne library because of its restraint in abstracting away all of the lower level theano functionality.  Within the context of the experiment, it was not only important to see the activations and weights at the layer level, but also to be able to grab and manipulate them.  Other popular neural net libraries do not provide this acccess out of the box.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the cortical network has 2 different training points, i.e. the *lower layer* weights with the hippocampal hidden layer activations, and the *upper layer* weights with the targets data outlined above; each piece of the cortical network had to built separately. This initial function defines the architecture of the lower cortical network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################## Build Lower Cortical Network #############################\n",
    "def build_cort_low_net(input_var=None):\n",
    "    l_input = lasagne.layers.InputLayer(shape=(None, 15),\n",
    "                                        input_var=input_var)\n",
    "    l_output = lasagne.layers.DenseLayer(\n",
    "                l_input,\n",
    "                num_units=40,\n",
    "                W=lasagne.init.Uniform(range=3.0),\n",
    "                nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    return l_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network consists of an **Input Layer** accepting a dataset of shape `(None, 15)` where the *None* parameter indicates the size of that dimension is not fixed.  This is followed by a fully connected **Dense Layer** with 40 nodes and a *rectify* activation function.  The weights matrix values are initialized as a random number between *-3 and 3* using [init.Uniform](https://lasagne.readthedocs.io/en/latest/modules/init.html#lasagne.init.Uniform) method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper cortical network is defined with similar layers to the lower cortical network with the following differences.\n",
    "1. The **Input Layer** accepts input with shape `(None, 40`) matching the output of the lower cortical network\n",
    "2. The **Dense Layer** consists of a single node, matching the desired network output\n",
    "3. A *sigmoid* activation function is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################## Build Upper Cortical Network #############################\n",
    "def build_cort_up_net(input_var=None):\n",
    "    l_input = lasagne.layers.InputLayer(shape=(None, 40),\n",
    "                                        input_var=input_var)\n",
    "    l_output = lasagne.layers.DenseLayer(\n",
    "                l_input,\n",
    "                num_units=1,\n",
    "                W=lasagne.init.Uniform(range=3.0),\n",
    "                nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    return l_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hippocampal network acts as a separate independent network.  It receives the same input vector/dataset as the cortical network, but unlike the cortical model; the hippocampal model acts an autoencoder, intending to recreate the the input as it's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Build Hippocampal Network ############################\n",
    "def build_hipp_net(input_var=None):\n",
    "    l_input = lasagne.layers.InputLayer(shape=(None, 15),\n",
    "                                        input_var=input_var)\n",
    "    l_hidden = lasagne.layers.DenseLayer(\n",
    "            l_input,\n",
    "            num_units=8,\n",
    "            W=lasagne.init.Uniform(range=3.0),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_output = lasagne.layers.DenseLayer(\n",
    "            l_hidden,\n",
    "            num_units=15,\n",
    "            nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    return l_hidden, l_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable differences from the cortical models:\n",
    "1. Since this network is self contained, there exists an input, hidden and output layer.  \n",
    "2. The # of nodes in the hidden layers is less the # of elements in the input. This is inentional and serves to *focus* the model on patterns within the connected weights. \n",
    "3. This network returns activations from both the hidden and output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model architectures are defined, we can define some helper functions that will run with the larger *run_net* function to produce output/feeder data along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_cort_net(num_batches, forward_func, update_func, data):\n",
    "    # instatiate empty lists that are needed\n",
    "    raw_out_list = []\n",
    "    for batch in range(num_batches):\n",
    "        forward_func(data)\n",
    "        raw_out_value = update_func(data)\n",
    "        raw_out_list.append(raw_out_value)\n",
    "    return raw_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_hipp_net(num_batches, forward_func, update_func, data):\n",
    "    # instatiate empty lists that are needed\n",
    "    raw_output_list = []\n",
    "    raw_hidden_list = []\n",
    "    for batch in range(num_batches):\n",
    "        forward_func(data)\n",
    "        raw_hid_value, raw_out_value = update_func(data)\n",
    "        raw_hidden_list.append(raw_hid_value)\n",
    "        raw_output_list.append(raw_out_value)\n",
    "    return raw_hidden_list, raw_output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *iter_cort_net* & *iter_hipp_net* functions handle the loop defined by the **N_BATCHES** constant.  Each pass through the loop represents a single feedforward and backward propogate action for a single dataset.  Since the variables being passed are *Theano shared variables* the values are remembered through the loops. In an ideal world these 2 functions would be combined into a single *iter_nets* function, a possible task in a refactoring round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_us_absent_present(index, out_list):\n",
    "    us_present_list = []\n",
    "    us_absent_list = []\n",
    "    for item in out_list:\n",
    "        us_present_list.append(float(item[index]))\n",
    "        try:\n",
    "            us_absent_list.append(float(item[index + 1]))\n",
    "        except IndexError:\n",
    "            us_absent_list.append(float(item[index - 1]))\n",
    "    return us_present_list, us_absent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *find_us_absent_present* function takes the output list from either *iter_&ast;_net* function and pulls at the output activations from the vector were the us was present, based on the cs_index variable from the *build_targets* function, and 1 representational vector were the cs was absent.  The representational vector is the vector immeditately preceeding the the us present, unless the us present is the first vector, in which case the vector immediately after the us present is selected.  This function is only used for the output of the upper cortical network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hid_abs_value(index, cort_list, hipp_list):\n",
    "    cort_us_absent_list = []\n",
    "    cort_us_present_list = []\n",
    "    hipp_us_present_list = []\n",
    "    hipp_us_absent_list = []\n",
    "    for item in cort_list:\n",
    "        cort_us_present_list.append(list(map(lambda x: abs(x), item[index])))\n",
    "        try:\n",
    "            cort_us_absent_list.append(list(map(lambda x: abs(x), item[index + 1])))\n",
    "        except IndexError:\n",
    "            cort_us_absent_list.append(list(map(lambda x: abs(x), item[index - 1])))\n",
    "    for item in hipp_list:\n",
    "        hipp_us_present_list.append(list(map(lambda x: abs(x), item[index])))\n",
    "        try:\n",
    "            hipp_us_absent_list.append(list(map(lambda x: abs(x), item[index + 1])))\n",
    "        except IndexError:\n",
    "            hipp_us_absent_list.append(list(map(lambda x: abs(x), item[index - 1])))\n",
    "    return cort_us_present_list, cort_us_absent_list, hipp_us_present_list, hipp_us_absent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the hidden layer of hippocampal network and the output of the lower cortical network could possibly hae negative values, it is imprtant to obtain absolute values for each activation list prior to additional processing. The *get_hid_abs_value* function pulls the us present & a representative us absent vectors, identical to above, but also converts each value to it's absolute value in the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) is a measure of difference between identical strings.  For our purposes, this measures the difference between the activation values for a vector where the us was present vs a vector where the us was absent. In practicality, it should steadily increase as the model *learns* when to predict the us being present, representing the difference between the activation values of both vector increasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hamm_dist(cort_abs_list, cort_pres_list, hipp_abs_list, hipp_pres_list):\n",
    "    c_dist_list = []\n",
    "    h_dist_list = []\n",
    "    for item in range(len(cort_pres_list)):\n",
    "        c_dist = np.absolute(np.subtract(np.asarray(cort_abs_list[item]), np.asarray(cort_pres_list[item])))\n",
    "        c_dist_list.append(np.sum(c_dist))\n",
    "    for item in range(len(hipp_pres_list)):\n",
    "        h_dist = np.absolute(np.subtract(np.asarray(hipp_abs_list[item]), np.asarray(hipp_pres_list[item])))\n",
    "        h_dist_list.append(np.sum(h_dist))\n",
    "    return c_dist_list, h_dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(cort_us_abs, cort_us_pres, c_dist, h_dist):\n",
    "    final_data = {\n",
    "        'X': cort_us_abs,\n",
    "        'XA': cort_us_pres,\n",
    "        'C-Dist': c_dist,\n",
    "        'H-Dist': h_dist,\n",
    "        }\n",
    "    net_output = pd.DataFrame(final_data, columns=final_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to leverage export to excel and plot functions, the final output data is dumped into a pandas dataset.  If you're using python 3.6, you have the added luxury of order dict out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(df_list, filename):\n",
    "    df_concat = pd.concat(df_list)\n",
    "    df_concat_by_index = df_concat.groupby(df_concat.index)\n",
    "    df_final = df_concat_by_index.mean().round(decimals=2)\n",
    "    xl_writer = pd.ExcelWriter(filename)\n",
    "    df_final.to_excel(xl_writer, 'Sheet1')\n",
    "    xl_writer.save()\n",
    "    df_final[['X', 'XA']].plot()\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up the helper functions:\n",
    "* *find_criterion* identifies threshold levels within the data\n",
    "* *run_sims* runs the larger simulations loop in which a single sub set of batches are ran\n",
    "* *convert_hipp_hidd_layer* convenrts the shape of the hippocampal hidden layer activations array to match the shape of the lower cortical layer output.  This is for training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_criterion(df, column, threshold):\n",
    "    try:\n",
    "        crit = df.loc[df[str(column)] >= threshold].index.values[0]\n",
    "    except IndexError:\n",
    "        return 'Criterion not reached'\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sims(num_sims, **kwargs):\n",
    "    df_list = []\n",
    "    for sim in range(num_sims):\n",
    "        df = run_nets(model=kwargs['model'],\n",
    "                        targets=kwargs['targets'],\n",
    "                        input_var=kwargs['input_var'],\n",
    "                        index=kwargs['index'],\n",
    "                        count=int(sim))\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hipp_hidd_layer(hipp_hidd_list):\n",
    "    return_list = [list(x) * 5 for x in hipp_hidd_list[len(hipp_hidd_list) - 1]]\n",
    "    # print(return_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the big boy, the actual *run_nets* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nets(model='i', **kwargs):\n",
    "    model_dict = {\n",
    "        'i': 'intact',\n",
    "        'l': 'lesion',\n",
    "        's': 'scopolamine',\n",
    "        'p': 'physostigmine',\n",
    "        }\n",
    "    # define theano shared variables for both networks\n",
    "    X_data_cort_low = T.matrix('X_data_cort_low')\n",
    "    X_data_cort_up = T.matrix('X_data_cort_up')\n",
    "    X_data_hipp = T.matrix('X_data_hipp')\n",
    "    # create nn models\n",
    "    print('Building networks based on {} model type, {} simulation...'.format(model_dict[str(model)], kwargs['count']))\n",
    "    cort_low_out_layer = build_cort_low_net(input_var=X_data_cort_low)\n",
    "    cort_low_out_formula = lasagne.layers.get_output(cort_low_out_layer)\n",
    "    cort_up_out_layer = build_cort_up_net(input_var=X_data_cort_up)\n",
    "    cort_up_out_formula = lasagne.layers.get_output(cort_up_out_layer)\n",
    "    hipp_hid_layer, hipp_out_layer = build_hipp_net(input_var=X_data_hipp)\n",
    "    hipp_hid_formula, hipp_out_formula = lasagne.layers.get_output([hipp_hid_layer, hipp_out_layer])\n",
    "    # branching point for different models based on model type\n",
    "    if model == 'i':\n",
    "        hipp_loss = lasagne.objectives.squared_error(hipp_out_formula, kwargs['input_var']).mean()\n",
    "        hipp_params = lasagne.layers.get_all_params(hipp_out_layer, trainable=True)\n",
    "        hipp_updates = lasagne.updates.momentum(hipp_loss, hipp_params, learning_rate=0.05, momentum=0.5)\n",
    "        feed_forward_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], allow_input_downcast=True)\n",
    "        back_update_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], updates=hipp_updates, allow_input_downcast=True)\n",
    "        hipp_hidd_list, hipp_out_list = iter_hipp_net(N_BATCHES, feed_forward_hipp, back_update_hipp, kwargs['input_var'])\n",
    "        cort_low_targets = convert_hipp_hidd_layer(hipp_hidd_list)\n",
    "        cort_up_loss = lasagne.objectives.binary_crossentropy(cort_up_out_formula, kwargs['targets'])\n",
    "        cort_up_loss = lasagne.objectives.aggregate(cort_up_loss, mode='mean')\n",
    "        cort_low_loss = lasagne.objectives.squared_error(cort_low_out_formula, cort_low_targets).mean()\n",
    "        cort_up_params = lasagne.layers.get_all_params(cort_up_out_layer, trainable=True)\n",
    "        cort_up_grads = theano.grad(cort_up_loss, wrt=cort_up_params)\n",
    "        cort_up_updates = lasagne.updates.sgd(cort_up_loss, cort_up_params, learning_rate=0.5)\n",
    "        cort_low_params = lasagne.layers.get_all_params(cort_low_out_layer, trainable=True)\n",
    "        cort_low_grads = theano.grad(cort_low_loss, wrt=cort_low_params)\n",
    "        cort_low_updates = lasagne.updates.sgd(cort_low_loss, cort_low_params, learning_rate=0.1)\n",
    "        feed_forward_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, allow_input_downcast=True)\n",
    "        feed_forward_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, allow_input_downcast=True)\n",
    "        back_update_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, updates=cort_low_updates, allow_input_downcast=True)\n",
    "        back_update_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, updates=cort_up_updates, allow_input_downcast=True)\n",
    "        cort_low_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_low, back_update_cort_low, kwargs['input_var'])\n",
    "        cort_up_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_up, back_update_cort_up, cort_low_out_list[len(cort_low_out_list)-1])\n",
    "        cort_us_present_up_out_list, cort_us_absent_up_out_list = find_us_absent_present(kwargs['index'], cort_up_out_list)\n",
    "        cort_us_present_low_out_list, cort_us_absent_low_out_list, hipp_us_present_hid_list, hipp_us_absent_hid_list = get_hid_abs_value(kwargs['index'], cort_low_out_list, hipp_hidd_list)\n",
    "        c_dist, h_dist = get_hamm_dist(cort_us_absent_low_out_list, cort_us_present_low_out_list, hipp_us_absent_hid_list, hipp_us_present_hid_list)\n",
    "        net_output = create_dataframe(cort_us_absent_up_out_list, cort_us_present_up_out_list, c_dist, h_dist)\n",
    "    elif model == 'p':\n",
    "        hipp_loss = lasagne.objectives.squared_error(hipp_out_formula, kwargs['input_var']).mean()\n",
    "        hipp_params = lasagne.layers.get_all_params(hipp_out_layer, trainable=True)\n",
    "        hipp_updates = lasagne.updates.momentum(hipp_loss, hipp_params, learning_rate=1.0, momentum=0.5)\n",
    "        feed_forward_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], allow_input_downcast=True)\n",
    "        back_update_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], updates=hipp_updates, allow_input_downcast=True)\n",
    "        hipp_hidd_list, hipp_out_list = iter_hipp_net(N_BATCHES, feed_forward_hipp, back_update_hipp, kwargs['input_var'])\n",
    "        cort_low_targets = convert_hipp_hidd_layer(hipp_hidd_list)\n",
    "        cort_up_loss = lasagne.objectives.binary_crossentropy(cort_up_out_formula, kwargs['targets'])\n",
    "        cort_up_loss = lasagne.objectives.aggregate(cort_up_loss, mode='mean')\n",
    "        cort_low_loss = lasagne.objectives.squared_error(cort_low_out_formula, cort_low_targets).mean()\n",
    "        cort_up_params = lasagne.layers.get_all_params(cort_up_out_layer, trainable=True)\n",
    "        cort_up_grads = theano.grad(cort_up_loss, wrt=cort_up_params)\n",
    "        cort_up_updates = lasagne.updates.sgd(cort_up_loss, cort_up_params, learning_rate=0.5)\n",
    "        cort_low_params = lasagne.layers.get_all_params(cort_low_out_layer, trainable=True)\n",
    "        cort_low_grads = theano.grad(cort_low_loss, wrt=cort_low_params)\n",
    "        cort_low_updates = lasagne.updates.sgd(cort_low_loss, cort_low_params, learning_rate=0.1)\n",
    "        feed_forward_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, allow_input_downcast=True)\n",
    "        feed_forward_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, allow_input_downcast=True)\n",
    "        back_update_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, updates=cort_low_updates, allow_input_downcast=True)\n",
    "        back_update_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, updates=cort_up_updates, allow_input_downcast=True)\n",
    "        cort_low_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_low, back_update_cort_low, kwargs['input_var'])\n",
    "        cort_up_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_up, back_update_cort_up, cort_low_out_list[len(cort_low_out_list)-1])\n",
    "        cort_us_present_up_out_list, cort_us_absent_up_out_list = find_us_absent_present(kwargs['index'], cort_up_out_list)\n",
    "        cort_us_present_low_out_list, cort_us_absent_low_out_list, hipp_us_present_hid_list, hipp_us_absent_hid_list = get_hid_abs_value(kwargs['index'], cort_low_out_list, hipp_hidd_list)\n",
    "        c_dist, h_dist = get_hamm_dist(cort_us_absent_low_out_list, cort_us_present_low_out_list, hipp_us_absent_hid_list, hipp_us_present_hid_list)\n",
    "        net_output = create_dataframe(cort_us_absent_up_out_list, cort_us_present_up_out_list, c_dist, h_dist)\n",
    "    elif model == 's':\n",
    "        hipp_loss = lasagne.objectives.squared_error(hipp_out_formula, kwargs['input_var']).mean()\n",
    "        hipp_params = lasagne.layers.get_all_params(hipp_out_layer, trainable=True)\n",
    "        hipp_updates = lasagne.updates.momentum(hipp_loss, hipp_params, learning_rate=0.001, momentum=0.5)\n",
    "        feed_forward_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], allow_input_downcast=True)\n",
    "        back_update_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], updates=hipp_updates, allow_input_downcast=True)\n",
    "        hipp_hidd_list, hipp_out_list = iter_hipp_net(N_BATCHES, feed_forward_hipp, back_update_hipp, kwargs['input_var'])\n",
    "        cort_low_targets = convert_hipp_hidd_layer(hipp_hidd_list)\n",
    "        cort_up_loss = lasagne.objectives.binary_crossentropy(cort_up_out_formula, kwargs['targets'])\n",
    "        cort_up_loss = lasagne.objectives.aggregate(cort_up_loss, mode='mean')\n",
    "        cort_low_loss = lasagne.objectives.squared_error(cort_low_out_formula, cort_low_targets).mean()\n",
    "        cort_up_params = lasagne.layers.get_all_params(cort_up_out_layer, trainable=True)\n",
    "        cort_up_grads = theano.grad(cort_up_loss, wrt=cort_up_params)\n",
    "        cort_up_updates = lasagne.updates.sgd(cort_up_loss, cort_up_params, learning_rate=0.5)\n",
    "        cort_low_params = lasagne.layers.get_all_params(cort_low_out_layer, trainable=True)\n",
    "        cort_low_grads = theano.grad(cort_low_loss, wrt=cort_low_params)\n",
    "        cort_low_updates = lasagne.updates.sgd(cort_low_loss, cort_low_params, learning_rate=0.1)\n",
    "        feed_forward_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, allow_input_downcast=True)\n",
    "        feed_forward_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, allow_input_downcast=True)\n",
    "        back_update_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, updates=cort_low_updates, allow_input_downcast=True)\n",
    "        back_update_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, updates=cort_up_updates, allow_input_downcast=True)\n",
    "        cort_low_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_low, back_update_cort_low, kwargs['input_var'])\n",
    "        cort_up_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_up, back_update_cort_up, cort_low_out_list[len(cort_low_out_list)-1])\n",
    "        cort_us_present_up_out_list, cort_us_absent_up_out_list = find_us_absent_present(kwargs['index'], cort_up_out_list)\n",
    "        cort_us_present_low_out_list, cort_us_absent_low_out_list, hipp_us_present_hid_list, hipp_us_absent_hid_list = get_hid_abs_value(kwargs['index'], cort_low_out_list, hipp_hidd_list)\n",
    "        c_dist, h_dist = get_hamm_dist(cort_us_absent_low_out_list, cort_us_present_low_out_list, hipp_us_absent_hid_list, hipp_us_present_hid_list)\n",
    "        net_output = create_dataframe(cort_us_absent_up_out_list, cort_us_present_up_out_list, c_dist, h_dist)\n",
    "    else:\n",
    "        cort_low_out_layer.params[cort_low_out_layer.W].remove('trainable')\n",
    "        cort_low_out_layer.params[cort_low_out_layer.b].remove('trainable')\n",
    "        hipp_loss = lasagne.objectives.squared_error(hipp_out_formula, kwargs['input_var']).mean()\n",
    "        hipp_params = lasagne.layers.get_all_params(hipp_out_layer, trainable=True)\n",
    "        hipp_updates = lasagne.updates.momentum(hipp_loss, hipp_params, learning_rate=0.05, momentum=0.5)\n",
    "        feed_forward_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], allow_input_downcast=True)\n",
    "        back_update_hipp = theano.function([X_data_hipp], [hipp_hid_formula, hipp_out_formula], updates=hipp_updates, allow_input_downcast=True)\n",
    "        hipp_hidd_list, hipp_out_list = iter_hipp_net(N_BATCHES, feed_forward_hipp, back_update_hipp, kwargs['input_var'])\n",
    "        cort_low_targets = convert_hipp_hidd_layer(hipp_hidd_list)\n",
    "        cort_up_loss = lasagne.objectives.binary_crossentropy(cort_up_out_formula, kwargs['targets'])\n",
    "        cort_up_loss = lasagne.objectives.aggregate(cort_up_loss, mode='mean')\n",
    "        cort_low_loss = lasagne.objectives.squared_error(cort_low_out_formula, cort_low_targets).mean()\n",
    "        cort_up_params = lasagne.layers.get_all_params(cort_up_out_layer, trainable=True)\n",
    "        cort_up_grads = theano.grad(cort_up_loss, wrt=cort_up_params)\n",
    "        cort_up_updates = lasagne.updates.sgd(cort_up_loss, cort_up_params, learning_rate=0.5)\n",
    "        cort_low_params = lasagne.layers.get_all_params(cort_low_out_layer, trainable=True)\n",
    "        cort_low_grads = theano.grad(cort_low_loss, wrt=cort_low_params)\n",
    "        cort_low_updates = lasagne.updates.sgd(cort_low_loss, cort_low_params, learning_rate=0.1)\n",
    "        feed_forward_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, allow_input_downcast=True)\n",
    "        feed_forward_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, allow_input_downcast=True)\n",
    "        back_update_cort_low = theano.function([X_data_cort_low], cort_low_out_formula, updates=cort_low_updates, allow_input_downcast=True)\n",
    "        back_update_cort_up = theano.function([X_data_cort_up], cort_up_out_formula, updates=cort_up_updates, allow_input_downcast=True)\n",
    "        cort_low_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_low, back_update_cort_low, kwargs['input_var'])\n",
    "        cort_up_out_list = iter_cort_net(N_BATCHES, feed_forward_cort_up, back_update_cort_up, cort_low_out_list[len(cort_low_out_list)-1])\n",
    "        cort_us_present_up_out_list, cort_us_absent_up_out_list = find_us_absent_present(kwargs['index'], cort_up_out_list)\n",
    "        cort_us_present_low_out_list, cort_us_absent_low_out_list, hipp_us_present_hid_list, hipp_us_absent_hid_list = get_hid_abs_value(kwargs['index'], cort_low_out_list, hipp_hidd_list)\n",
    "        c_dist, h_dist = get_hamm_dist(cort_us_absent_low_out_list, cort_us_present_low_out_list, hipp_us_absent_hid_list, hipp_us_present_hid_list)\n",
    "        net_output = create_dataframe(cort_us_absent_up_out_list, cort_us_present_up_out_list, c_dist, h_dist)\n",
    "    return net_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the meat of the application and houses the actual training and prediction side of the model.  Lets deconstruct the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally to run some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, grab user input for which model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select model type: intact(i), lesion(l), phystogimine(p), scopolomine(s): l\n"
     ]
    }
   ],
   "source": [
    "user_response = input('Select model type: intact(i), lesion(l), phystogimine(p), scopolomine(s): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & view the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "input_var = build_dataset()\n",
    "print(input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build & view the targets, note the location of the *1* in the targets matches that of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "targets, cs_index = build_targets(input_var)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally...running the network! Depending on your hardware, this can take up to 10 minutes; I warned you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building networks based on lesion model type, 0 simulation...\n",
      "Building networks based on lesion model type, 1 simulation...\n",
      "Building networks based on lesion model type, 2 simulation...\n",
      "Building networks based on lesion model type, 3 simulation...\n",
      "Building networks based on lesion model type, 4 simulation...\n",
      "Building networks based on lesion model type, 5 simulation...\n",
      "Building networks based on lesion model type, 6 simulation...\n",
      "Building networks based on lesion model type, 7 simulation...\n",
      "Building networks based on lesion model type, 8 simulation...\n",
      "Building networks based on lesion model type, 9 simulation...\n",
      "Building networks based on lesion model type, 10 simulation...\n",
      "Building networks based on lesion model type, 11 simulation...\n",
      "Building networks based on lesion model type, 12 simulation...\n",
      "Building networks based on lesion model type, 13 simulation...\n",
      "Building networks based on lesion model type, 14 simulation...\n",
      "Building networks based on lesion model type, 15 simulation...\n",
      "Building networks based on lesion model type, 16 simulation...\n",
      "Building networks based on lesion model type, 17 simulation...\n",
      "Building networks based on lesion model type, 18 simulation...\n",
      "Building networks based on lesion model type, 19 simulation...\n"
     ]
    }
   ],
   "source": [
    "df_list = run_sims(N_SIMS,\n",
    "                    model=user_response,\n",
    "                    targets=targets,\n",
    "                    input_var=input_var,\n",
    "                    index=cs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All objects passed were None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-394613a923f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-f949e2f8a2fd>\u001b[0m in \u001b[0;36mcreate_output\u001b[1;34m(df_list, filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdf_concat_by_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_concat_by_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mxl_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adam\\docume~1\\python~1\\projects\\nn_pro~1\\nn_proj\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adam\\docume~1\\python~1\\projects\\nn_pro~1\\nn_proj\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'All objects passed were None'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# consolidate data & figure out what our result ndim is going to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All objects passed were None"
     ]
    }
   ],
   "source": [
    "df_final = create_output(df_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the plot inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
